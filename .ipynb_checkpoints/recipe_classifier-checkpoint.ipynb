{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb3263b-7c05-455a-8867-6b6b95511226",
   "metadata": {},
   "source": [
    "# ğŸ½ï¸ğŸ³ Ù…ØµÙ†Ù ÙˆØµÙØ§Øª Ø§Ù„Ø·Ø¹Ø§Ù… \n",
    "### ğŸ” ğŸ• ğŸ— ğŸ– ğŸ¥© ğŸ¤ ğŸ› ğŸ£ ğŸŒ® ğŸ¥™ ğŸ ğŸ§€ ğŸ§ ğŸ© ğŸª ğŸ« ğŸ¯ ğŸ ğŸ‰ ğŸ‡ ğŸ“ ğŸ¥ ğŸ ğŸ¥¥ ğŸ¥‘ ğŸ¥• ğŸŒ½ ğŸ¥” ğŸ¥¬ ğŸµ â˜• ğŸœ ğŸ² ğŸš ğŸ™ ğŸ˜ ğŸ¥Ÿ ğŸ¥  ğŸ¥¡ ğŸ¥£ ğŸ¥ ğŸ§‡ ğŸ® ğŸ¨ ğŸ¥ ğŸ§‡ ğŸ® ğŸ¢ ğŸ¡ ğŸ§ ğŸŒ° ğŸ  ğŸ«” ğŸ«“ ğŸŒ° ğŸ  ğŸ«” ğŸ«“ ğŸ§† ğŸ«• ğŸ«’ ğŸ§ƒ ğŸŒ¶ ğŸ¥œ\n",
    "\n",
    "### **Ù…Ø´Ø±ÙˆØ¹ ÙŠÙ‡Ø¯Ù Ø§Ù„Ù‰ ØªØµÙ†ÙŠÙ ÙˆØµÙØ§Øª Ø§Ù„Ø·Ø¹Ø§Ù… Ø§Ù„Ù…Ø¯Ø®Ù„Ø© Ø§Ù„Ù‰ Ø§Ø­Ø¯ Ø§Ù„Ø³Ø¨Ø¹ Ø§ØµÙ†Ø§Ù ğŸ¯**\n",
    "- **Ø§Ø·Ø¨Ø§Ù‚ Ø±Ø¦ÙŠØ³ÙŠØ© ğŸ**\n",
    "- **Ù…Ù‚Ø¨Ù„Ø§Øª ğŸŸ**\n",
    "- **Ø­Ø³Ø§Ø¡ ğŸ²**\n",
    "- **Ø³Ù„Ø·Ø§Øª ğŸ¥—**\n",
    "- **Ø³Ø§Ù†Ø¯ÙˆÙŠØªØ´Ø§Øª ğŸ¥ª**\n",
    "- **Ø­Ù„ÙˆÙŠØ§Øª ğŸ°**\n",
    "- **Ù…Ø´Ø±ÙˆØ¨Ø§Øª ğŸ¥¤**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54fd9a0d-58ca-4c5b-97b7-280a1868fae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk import FreqDist \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f60e28-fbed-4ae4-86a3-74482f0fb250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø­Ø°Ù Ø§Ù„ØªØ´ÙˆÙŠØ´ \n",
    "def noise_removal(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # Ø­Ø°Ù ÙˆØ³ÙˆÙ… HTML\n",
    "    text = re.sub(r'http\\S+', '', text)  # Ø­Ø°Ù URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Ø­Ø°Ù Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙ†ØµÙŠØµ\n",
    "    text = re.sub(r'\\d+', '', text)  # Ø­Ø°Ù Ø§Ù„Ø§Ø±Ù‚Ø§Ù…\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a36e66a-ffb7-43b7-b4ed-85e6193b16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ø­Ø°Ù Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ Ù„Ø§ ØªØ¤Ø«Ø± ÙÙŠ Ø§Ù„Ù…Ø¹Ù†Ù‰\n",
    "def removing_stop_words(words):\n",
    "\n",
    "    arabic_stopwords = set(stopwords.words('arabic'))\n",
    "    \n",
    "    filtered_words = [word for word in words if word not in arabic_stopwords]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46f3a919-09f6-4915-b63d-9b3ccb6c3451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø§Ø±Ø¬Ø§Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„Ø¬Ø°Ø±Ù‡Ø§\n",
    "def stemming(words):\n",
    "    stemmer = ISRIStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42d763a7-0fa4-4c41-bc27-3815239034ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text):\n",
    "    text = noise_removal(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = removing_stop_words(tokens)\n",
    "    tokens = stemming(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e6accc1-7cff-485a-b002-01f7418e0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateBOW(wordset,doc):\n",
    "    # Ø§Ù†Ø´Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±ÙŠØ¯Ù‡ ÙˆØªÙƒØ±Ø§Ø±Ù‡\n",
    "  fdist = FreqDist(doc)\n",
    "  return {word: fdist[word] for word in wordset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77f6be01-9648-4ff8-b095-a6bc337f3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = 'C:\\\\Users\\\\LENOVO\\\\Desktop\\\\recipe_classifier\\\\recipes_corpus'\n",
    "corpus = {}\n",
    "\n",
    "for file_name in os.listdir(corpus_root):\n",
    "    #Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…Ù„Ù\n",
    "    label = file_name.split('.')[0] #Ø­Ù„Ù‰.txt ['Ø­Ù„Ù‰','txt']\n",
    "    with open(os.path.join(corpus_root, file_name), 'r', encoding='utf-8') as file:\n",
    "        corpus[label] = file.read()\n",
    "\n",
    "\n",
    "processed_corpus = {}\n",
    "for label, text in corpus.items():\n",
    "    processed_corpus[label] = text_processing(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17e3103b-2e7f-46f3-867c-9e8fa354e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª\n",
    "freq = {}\n",
    "for label in processed_corpus:\n",
    "    freq[label] = Counter(processed_corpus[label]) \n",
    "\n",
    "# Ø­Ø³Ø§Ø¨ Ø§Ø¹Ù„Ù‰ 100 ÙƒÙ„Ù…Ø© Ø¸Ù‡Ø±Øª ÙÙŠ ÙƒÙ„ ØªØµÙ†ÙŠÙ\n",
    "top_words = {}\n",
    "for label in processed_corpus:\n",
    "    top_words[label] = [word for word, _ in freq[label].most_common(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1af07b12-77ec-4268-9ee3-401e3ec350ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ù‚Ø§Ø¦Ù…Ø© ØªØ¬Ù…Ø¹ Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ÙˆØµÙØ§Øª\n",
    "documents = [processed_corpus[label] for label in processed_corpus]\n",
    "\n",
    "# Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø§ØµÙ†Ø§Ù\n",
    "labels = [label for label in processed_corpus]\n",
    "\n",
    "# Ù‚Ø§Ø¦Ù…Ø© Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø©\n",
    "wordset = np.unique([word for doc in documents for word in doc])\n",
    "\n",
    "bow_dicts = [calculateBOW(wordset, doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "281e4c4c-d833-48bb-9e48-d482e7f8acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø¯Ø§Ù„Ø© ØªÙ‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª ÙˆÙ‚Ø§Ù…ÙˆØ³ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…\n",
    "\n",
    "def classify_recipes(bow_dict): #ÙŠÙØ¹Ø±Ù‘Ù Ø¯Ø§Ù„Ø© Ø§Ø³Ù…Ù‡Ø§ classify_recipes ØªØ£Ø®Ø° Ù‚Ø§Ù…ÙˆØ³ bow_dict (Bag of Words) Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù…Ù† ÙˆØµÙØ© ÙˆØ¹Ø¯Ø¯ ØªÙƒØ±Ø§Ø± ÙƒÙ„ ÙƒÙ„Ù…Ø©\n",
    "    scores = {} #ÙŠÙ†Ø´Ø¦ Ù‚Ø§Ù…ÙˆØ³ scores Ù„Ø­ÙØ¸ \"Ø§Ù„Ø¯Ø±Ø¬Ø©\" Ù„ÙƒÙ„ ØªØµÙ†ÙŠÙ. ÙƒÙ„ ØªØµÙ†ÙŠÙ ÙŠØ­ØµÙ„ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹ Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª Ø¸Ù‡ÙˆØ± ÙƒÙ„Ù…Ø§ØªÙ‡ Ø§Ù„Ù…Ù…ÙŠØ²Ø©.\n",
    "    \n",
    "    for label in top_words: #Ù†Ø¨Ø¯Ø£ Ø­Ù„Ù‚Ø© ØªÙ…Ø± Ø¹Ù„Ù‰ ÙƒÙ„ ØªØµÙ†ÙŠÙ (Ù…Ø«Ù„Ø§Ù‹: \"Ø­Ù„Ù‰\"ØŒ \"Ø·Ø¨Ù‚ Ø±Ø¦ÙŠØ³ÙŠ\"ØŒ \"Ù…Ù‚Ø¨Ù„Ø§Øª\").\n",
    "#top_words ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù…ÙŠØ²Ø© Ù„ÙƒÙ„ ØªØµÙ†ÙŠÙ.\n",
    "\n",
    "        scores[label] = sum(bow_dict[word] for word in top_words[label] if word in bow_dict) #Ù†Ø­Ø³Ø¨ Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù…ÙŠØ²Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙØ¹Ù„Ù‹Ø§ ÙÙŠ bow_dict.\n",
    "#ÙŠØ¹Ù†ÙŠ: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙˆØµÙØ© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª ÙƒØ«ÙŠØ±Ø© ØªÙ†ØªÙ…ÙŠ Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙ \"Ø·Ø¨Ù‚ Ø±Ø¦ÙŠØ³ÙŠ\"ØŒ ÙŠØ­ØµÙ„ \"Ø·Ø¨Ù‚ Ø±Ø¦ÙŠØ³ÙŠ\" Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹ Ø£ÙƒØ¨Ø±\n",
    "   \n",
    "    return max(scores, key=scores.get) #ØªØ±Ø¬Ø¹ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø°ÙŠ Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø£Ø¹Ù„Ù‰ Ø¯Ø±Ø¬Ø©ØŒ Ø£ÙŠ Ø£Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙˆØµÙØ©.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5768c5f3-b468-4f31-9932-b52293a051d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø£Ø¯Ø®Ù„ Ø®Ø·ÙˆØ§Øª ÙˆØµÙØ© Ù„ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹Ù‡Ø§ (Ø­Ù„Ù‰ØŒ Ø·Ø¨Ù‚ Ø±Ø¦ÙŠØ³ÙŠØŒ Ù…Ù‚Ø¨Ù„Ø§ØªØŒ ...):  Ø¡. Ù†Ø¹Ø¬Ù† Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø¬ÙŠØ¯Ù‹Ø§ Ø­ØªÙ‰ ØªØ®ØªÙ„Ø·. Ù†ØªØ±Ùƒ Ø§Ù„Ø¹Ø¬ÙŠÙ†Ø© Ù„ØªØ±ØªØ§Ø­. ÙÙŠ ÙˆØ¹Ø§Ø¡ Ø§Ù„Ø®Ù„Ø§Ø· Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØŒ Ù†Ø¶Ø¹ Ø§Ù„Ø¨Ø·Ø§Ø·Ø³ØŒ Ø§Ù„Ù…Ù„Ø­ØŒ Ø§Ù„ÙÙ„ÙÙ„ Ø§Ù„Ø£Ø³ÙˆØ¯ØŒ Ø§Ù„ÙÙ„ÙÙ„ Ø§Ù„Ø­Ø§Ø±ØŒ Ø§Ù„ØºØ§Ø±Ø§Ù… Ù…Ø§Ø³Ø§Ù„Ø§ØŒ Ø§Ù„ÙƒÙ…ÙˆÙ† ÙˆØ§Ù„Ø­Ù…Ù‘Øµ. Ù†Ø®Ù„Ø· Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø¬ÙŠØ¯Ù‹Ø§ ÙÙŠ Ø§Ù„Ø®Ù„Ø§Ø·. Ù†ÙØ±Ø¯ Ø§Ù„Ø¹Ø¬ÙŠÙ†Ø© Ø¹Ù„Ù‰ Ø³Ø·Ø­ ØµÙ„Ø¨ Ø«Ù…Ù‘ Ù†Ù‚Ø·Ù‘Ø¹Ù‡Ø§ Ø¥Ù„Ù‰ Ø¯ÙˆØ§Ø¦Ø±. ÙÙŠ Ø²ÙŠØª ØºØ²ÙŠØ± Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø§Ø± Ù†Ù‚Ù„ÙŠ Ø§Ù„Ø¹Ø¬ÙŠÙ† Ø­ØªÙ‰ ÙŠÙ†ØªÙØ®. Ù†Ø­Ø´ÙŠ Ø§Ù„Ø¹Ø¬ÙŠÙ† Ø¨ÙƒÙ…ÙŠØ© Ù…Ù† Ø§Ù„Ø­Ø´ÙˆØ© Ø«Ù…Ù‘ Ù†Ù‚Ø¯Ù‘Ù… Ø§Ù„Ù…Ù‚Ø¨Ù„Ø§Øª Ù…Ø¹ Ø§Ù„ØµÙˆØµ Ø§Ù„Ø­Ø§Ø±. Ù†Ø®Ù„Ø· Ø§Ù„Ø¨Ø·Ø§Ø·Ø³ Ù…Ø¹ Ø§Ù„Ù…Ù„Ø­ØŒ Ø§Ù„ÙÙ„ÙÙ„ Ø§Ù„Ø£Ø³ÙˆØ¯ ÙˆØ§Ù„Ù†Ø´Ø§Ø¡ ÙˆÙ†Ø¹Ø¬Ù† Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø­ØªÙ‰ ÙŠØµØ¨Ø­ Ø§Ù„Ø®Ù„ÙŠØ· Ù…Ø«Ù„ Ù‚ÙˆØ§Ù… Ø§Ù„Ø¹Ø¬ÙŠÙ†Ø©. Ù†Ø£Ø®Ø° ÙƒÙ…ÙŠØ© Ù…Ù† Ø§Ù„Ø®Ù„ÙŠØ· Ø«Ù…Ù‘ Ù†Ø­Ø´ÙŠÙ‡ Ø¨Ø§Ù„Ø¬Ø¨Ù†. Ù†ØºÙ„Ù‘Ù Ø§Ù„Ø£ØµØ§Ø¨Ø¹ Ø¨Ø§Ù„Ù†Ø´Ø§Ø¡ØŒ Ø¨Ø¹Ø¯Ù‡Ø§ Ø¨Ø§Ù„Ø¨ÙŠØ¶ ÙˆØ¨Ø¹Ø¯Ù‡Ø§ Ø§Ù„Ø¨Ù‚Ø³Ù…Ø§Ø·. Ù†Ù‚Ù„ÙŠ Ø£ØµØ§Ø¨Ø¹ Ø§Ù„Ø¨Ø·Ø§Ø·Ø³ Ø¨Ø§Ù„Ø²ÙŠØª Ø§Ù„Ù†Ø¨Ø§ØªÙŠ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ØªØµÙ†ÙŠÙ Ø§Ù„ÙˆØµÙØ©: Ù…Ù‚Ø¨Ù„Ø§Øª\n"
     ]
    }
   ],
   "source": [
    "# Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…\n",
    "user_text = input(\"Ø£Ø¯Ø®Ù„ Ø®Ø·ÙˆØ§Øª ÙˆØµÙØ© Ù„ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹Ù‡Ø§ (Ø­Ù„Ù‰ØŒ Ø·Ø¨Ù‚ Ø±Ø¦ÙŠØ³ÙŠØŒ Ù…Ù‚Ø¨Ù„Ø§ØªØŒ ...): \")\n",
    "\n",
    "# Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…\n",
    "user_tokens = text_processing(user_text)\n",
    "\n",
    "\n",
    "# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ù†Ø³Ø¨\n",
    "predicted_class = classify_recipes(calculateBOW(wordset,user_tokens))\n",
    "print(\"ğŸ” ØªØµÙ†ÙŠÙ Ø§Ù„ÙˆØµÙØ©:\", predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
