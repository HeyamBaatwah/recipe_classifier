{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb3263b-7c05-455a-8867-6b6b95511226",
   "metadata": {},
   "source": [
    "# 🍽️🍳 مصنف وصفات الطعام \n",
    "### 🍔 🍕 🍗 🍖 🥩 🍤 🍛 🍣 🌮 🥙 🍞 🧀 🧁 🍩 🍪 🍫 🍯 🍎 🍉 🍇 🍓 🥝 🍍 🥥 🥑 🥕 🌽 🥔 🥬 🍵 ☕ 🍜 🍲 🍚 🍙 🍘 🥟 🥠 🥡 🥣 🥞 🧇 🍮 🍨 🥞 🧇 🍮 🍢 🍡 🍧 🌰 🍠 🫔 🫓 🌰 🍠 🫔 🫓 🧆 🫕 🫒 🧃 🌶 🥜\n",
    "\n",
    "### **مشروع يهدف الى تصنيف وصفات الطعام المدخلة الى احد السبع اصناف 🎯**\n",
    "- **اطباق رئيسية 🍝**\n",
    "- **مقبلات 🍟**\n",
    "- **حساء 🍲**\n",
    "- **سلطات 🥗**\n",
    "- **ساندويتشات 🥪**\n",
    "- **حلويات 🍰**\n",
    "- **مشروبات 🥤**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54fd9a0d-58ca-4c5b-97b7-280a1868fae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk import FreqDist \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f60e28-fbed-4ae4-86a3-74482f0fb250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# حذف التشويش \n",
    "def noise_removal(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # حذف وسوم HTML\n",
    "    text = re.sub(r'http\\S+', '', text)  # حذف URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # حذف علامات التنصيص\n",
    "    text = re.sub(r'\\d+', '', text)  # حذف الارقام\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a36e66a-ffb7-43b7-b4ed-85e6193b16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#حذف الكلمات التي لا تؤثر في المعنى\n",
    "def removing_stop_words(words):\n",
    "\n",
    "    arabic_stopwords = set(stopwords.words('arabic'))\n",
    "    \n",
    "    filtered_words = [word for word in words if word not in arabic_stopwords]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46f3a919-09f6-4915-b63d-9b3ccb6c3451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ارجاع الكلمات لجذرها\n",
    "def stemming(words):\n",
    "    stemmer = ISRIStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42d763a7-0fa4-4c41-bc27-3815239034ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text):\n",
    "    text = noise_removal(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = removing_stop_words(tokens)\n",
    "    tokens = stemming(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e6accc1-7cff-485a-b002-01f7418e0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateBOW(wordset,doc):\n",
    "    # انشاء قاموس للكلمات الفريده وتكراره\n",
    "  fdist = FreqDist(doc)\n",
    "  return {word: fdist[word] for word in wordset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77f6be01-9648-4ff8-b095-a6bc337f3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = 'C:\\\\Users\\\\LENOVO\\\\Desktop\\\\recipe_classifier\\\\recipes_corpus'\n",
    "corpus = {}\n",
    "\n",
    "for file_name in os.listdir(corpus_root):\n",
    "    #عنوان الملف\n",
    "    label = file_name.split('.')[0] #حلى.txt ['حلى','txt']\n",
    "    with open(os.path.join(corpus_root, file_name), 'r', encoding='utf-8') as file:\n",
    "        corpus[label] = file.read()\n",
    "\n",
    "\n",
    "processed_corpus = {}\n",
    "for label, text in corpus.items():\n",
    "    processed_corpus[label] = text_processing(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17e3103b-2e7f-46f3-867c-9e8fa354e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# حساب عدد تكرار الكلمات\n",
    "freq = {}\n",
    "for label in processed_corpus:\n",
    "    freq[label] = Counter(processed_corpus[label]) \n",
    "\n",
    "# حساب اعلى 100 كلمة ظهرت في كل تصنيف\n",
    "top_words = {}\n",
    "for label in processed_corpus:\n",
    "    top_words[label] = [word for word, _ in freq[label].most_common(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1af07b12-77ec-4268-9ee3-401e3ec350ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# قائمة تجمع قوائم الوصفات\n",
    "documents = [processed_corpus[label] for label in processed_corpus]\n",
    "\n",
    "# قائمة بالاصناف\n",
    "labels = [label for label in processed_corpus]\n",
    "\n",
    "# قائمة للكلمات الفريدة\n",
    "wordset = np.unique([word for doc in documents for word in doc])\n",
    "\n",
    "bow_dicts = [calculateBOW(wordset, doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "281e4c4c-d833-48bb-9e48-d482e7f8acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# دالة تقارن بين التصنيفات وقاموس المستخدم\n",
    "\n",
    "def classify_recipes(bow_dict): #يُعرّف دالة اسمها classify_recipes تأخذ قاموس bow_dict (Bag of Words) الذي يحتوي على الكلمات من وصفة وعدد تكرار كل كلمة\n",
    "    scores = {} #ينشئ قاموس scores لحفظ \"الدرجة\" لكل تصنيف. كل تصنيف يحصل على مجموع عدد مرات ظهور كلماته المميزة.\n",
    "    \n",
    "    for label in top_words: #نبدأ حلقة تمر على كل تصنيف (مثلاً: \"حلى\"، \"طبق رئيسي\"، \"مقبلات\").\n",
    "#top_words يجب أن يكون قاموس يحتوي على الكلمات المميزة لكل تصنيف.\n",
    "\n",
    "        scores[label] = sum(bow_dict[word] for word in top_words[label] if word in bow_dict) #نحسب مجموع التكرارات للكلمات المميزة لهذا التصنيف الموجودة فعلًا في bow_dict.\n",
    "#يعني: إذا كانت الوصفة تحتوي على كلمات كثيرة تنتمي إلى تصنيف \"طبق رئيسي\"، يحصل \"طبق رئيسي\" على مجموع أكبر\n",
    "   \n",
    "    return max(scores, key=scores.get) #ترجع التصنيف الذي حصل على أعلى درجة، أي أعلى تطابق مع كلمات الوصفة.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5768c5f3-b468-4f31-9932-b52293a051d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أدخل خطوات وصفة لتحديد نوعها (حلى، طبق رئيسي، مقبلات، ...):  ء. نعجن المكونات جيدًا حتى تختلط. نترك العجينة لترتاح. في وعاء الخلاط الكهربائي، نضع البطاطس، الملح، الفلفل الأسود، الفلفل الحار، الغارام ماسالا، الكمون والحمّص. نخلط المكونات جيدًا في الخلاط. نفرد العجينة على سطح صلب ثمّ نقطّعها إلى دوائر. في زيت غزير على النار نقلي العجين حتى ينتفخ. نحشي العجين بكمية من الحشوة ثمّ نقدّم المقبلات مع الصوص الحار. نخلط البطاطس مع الملح، الفلفل الأسود والنشاء ونعجن المكونات حتى يصبح الخليط مثل قوام العجينة. نأخذ كمية من الخليط ثمّ نحشيه بالجبن. نغلّف الأصابع بالنشاء، بعدها بالبيض وبعدها البقسماط. نقلي أصابع البطاطس بالزيت النباتي\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 تصنيف الوصفة: مقبلات\n"
     ]
    }
   ],
   "source": [
    "# نص المستخدم\n",
    "user_text = input(\"أدخل خطوات وصفة لتحديد نوعها (حلى، طبق رئيسي، مقبلات، ...): \")\n",
    "\n",
    "# معالجة نص المستخدم\n",
    "user_tokens = text_processing(user_text)\n",
    "\n",
    "\n",
    "# تحديد التصنيف الأنسب\n",
    "predicted_class = classify_recipes(calculateBOW(wordset,user_tokens))\n",
    "print(\"🔎 تصنيف الوصفة:\", predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
